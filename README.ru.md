# polyTron 
Библиотека для детального построения многослойных нейросетей.
##Технические особенности

- Динамическое изменение размера нейронов и слоев.
- Возможность задать функцию активации для слоя или отдельного нейрона.
- Возможность задать "рейт" для слоя или отдельного нейрона.
- Функция ускорения обучения сети (Хmax) так же ee возможно задать для слоя или отдельного нейрона.
- Возможность доучивать сеть после обучения.
- Обучение производится по одному примеру за один раз.
- Атрибут "эпоха" отсутствует, но его легко реализовать на этапе обучения сети.
- Одномерный массив входящих данных.
- Экспорт,импорт связей сети в JSON формат.
- Возможность запуска под браузером и сервером node.js.

###Активационные функции
relu, log, abs, htan2, tanh, atan, sin, htan, gaus

##Использование на примере задачи xor
###Подключение в браузере:
```html
<script src="/path/to/polytron.js"></script>
``` 
###Подключение в node.js:
```javascript
var Polytron = require('/path/to/polytron.js').Polytron;
``` 
###Создание слоя и инициализация объекта:
В данном варианте создается обычная сеть с сигмоидной функцией активации (log) и общим для всех слоев рейтом (0.5)
```javascript
var layers=[
{"size":2, "model":"input",  "rate":0  , "active":"none"},
{"size":4, "model":"hidden", "rate":0.5, "active":"log"},
{"size":1, "model":"output", "rate":0.5, "active":"log"}
		   ];
``` 
Можно задать для выходного слоя индивидуальный rate и индивидуальную функцию активации  
```javascript
var layers=[
{"size":2, "model":"input",  "rate":0  , "active":"none"},
{"size":4, "model":"hidden", "rate":0.5, "active":"log"},
{"size":1, "model":"output", "rate":0.1, "active":"relu"}
		   ];
``` 
Также можно значительно ускорить обучение сети добавив атрибут "Xmax", 
50 значит на сколько будет умножаться rate для коррекции максимального положительного и отрицательного значения ошибки  
```javascript
var layers=[
{"size":2, "model":"input",  "rate":0  , "active":"none"},
{"size":4, "model":"hidden", "rate":0.5, "active":"log", "Xmax":50},
{"size":1, "model":"output", "rate":0.1, "active":"relu"}
		   ];
``` 
Сеть поддерживает детализацию активационных функций, rate, Xmax для каждого нейрона т.е
в этом примере нейроны сети будут иметь разную активационню функцию, а так же разный коэффициент шага и разное ускорение
```javascript
var layers=[
{"size":2, "model":"input",  "rate":0  , "active":"none"},
{"size":5, "model":"hidden", "rate":[0.2,0.3,0.5,0.3,0.2], "active":["log","abs","tanh","abs","log"], "Xmax":[2,3,50,3,2]},
{"size":1, "model":"output", "rate":0.1, "active":"relu"}
		   ];
``` 
Также в сети имеется фунция наследования от рядом стоящего нейрона, т.е последнее значение в массивах
распространится на послдующие нейроны
```javascript
var layers=[
{"size":2, "model":"input",  "rate":0  , "active":"none"},
{"size":5, "model":"hidden", "rate":[0.2,0.3], "active":["abs","log"], "Xmax":[2,3]},
{"size":1, "model":"output", "rate":0.1, "active":"relu"}
		   ];
``` 

###Инициализация объекта
```javascript
var nn = new Polytron(layers);
``` 
С загрузкой весов из src (json) сохраненного скажем после первой итерации
```javascript
var nn = new Polytron(layers);
nn.load(src);
``` 


###Обучение

```javascript
var inp=[];
inp[0]={input:[0, 0],target:[0.9]};
inp[1]={input:[0, 1],target:[0.5]};
inp[2]={input:[1, 0],target:[0.5]};
inp[3]={input:[1, 1],target:[0.9]};	//Подготовим обучающую выборку
var epoch=1000; 				 	//Количество эпох
var err_sum=0; 					 	//Сумма ошибок
var minerror=0.0000000000000001; 	//Минимальная ошибка
var it=0; 						 	//Итераций пройдено
while(epoch>0){
	err_sum=0;
for(var i=0;i<inp.length;i++){
	var out=nn.forward(inp[i].input); 		 //Кормим сеть выборкой (обязательно перед обучением)
	nn.backward(inp[i].input,inp[i].target); //учим сеть
	err_sum+=nn.mse(inp[i].target,out);      //Суммируем разницу
							 }
if(Math.abs(err_sum)<=minerror){break;}      //Если сумма ошибки меньше или равна выходим из цикла
epoch--; it++;
			  }
```

###Экспорт весовых коэффициент
После обучения можно сохранить веса в контейнер или в файл а после использовать при дальнейшем обучении
```javascript
var src = nn.toString();
```
 
###Проверка правильности
```javascript
var out=nn.forward([1, 0]); //Кормим пример
console.log("target: 0.5"); //Цель 
console.log("input: "+[1, 0].join()); //Выводим
console.log("out:"+out); 	//Получем результат
```

## Лицензия
Лицензия MIT (MIT)
Copyright (c) 2016 S.S.Korotaev

Данная лицензия разрешает лицам, получившим копию данного программного обеспечения и сопутствующей документации (в дальнейшем именуемыми «Программное Обеспечение»), безвозмездно использовать Программное Обеспечение без ограничений, включая неограниченное право на использование, копирование, изменение, слияние, публикацию, распространение, сублицензирование и/или продажу копий Программного Обеспечения, а также лицам, которым предоставляется данное Программное Обеспечение, при соблюдении следующих условий:

Указанное выше уведомление об авторском праве и данные условия должны быть включены во все копии или значимые части данного Программного Обеспечения.

ДАННОЕ ПРОГРАММНОЕ ОБЕСПЕЧЕНИЕ ПРЕДОСТАВЛЯЕТСЯ «КАК ЕСТЬ», БЕЗ КАКИХ-ЛИБО ГАРАНТИЙ, ЯВНО ВЫРАЖЕННЫХ ИЛИ ПОДРАЗУМЕВАЕМЫХ, ВКЛЮЧАЯ ГАРАНТИИ ТОВАРНОЙ ПРИГОДНОСТИ, СООТВЕТСТВИЯ ПО ЕГО КОНКРЕТНОМУ НАЗНАЧЕНИЮ И ОТСУТСТВИЯ НАРУШЕНИЙ, НО НЕ ОГРАНИЧИВАЯСЬ ИМИ. НИ В КАКОМ СЛУЧАЕ АВТОРЫ ИЛИ ПРАВООБЛАДАТЕЛИ НЕ НЕСУТ ОТВЕТСТВЕННОСТИ ПО КАКИМ-ЛИБО ИСКАМ, ЗА УЩЕРБ ИЛИ ПО ИНЫМ ТРЕБОВАНИЯМ, В ТОМ ЧИСЛЕ, ПРИ ДЕЙСТВИИ КОНТРАКТА, ДЕЛИКТЕ ИЛИ ИНОЙ СИТУАЦИИ, ВОЗНИКШИМ ИЗ-ЗА ИСПОЛЬЗОВАНИЯ ПРОГРАММНОГО ОБЕСПЕЧЕНИЯ ИЛИ ИНЫХ ДЕЙСТВИЙ С ПРОГРАММНЫМ ОБЕСПЕЧЕНИЕМ.
